metrics = c("users", "sessions","pageviews"),
dimensions = "date",
anti_sample = TRUE,
segments = NULL,
anti_sample_batches = 1)
View(ga_data)
start_date <- "2018-08-01"
end_date <- "2018-08-31"
ga_data <- google_analytics(my_id,
date_range = c(start_date, end_date),
metrics = c("users", "sessions","pageviews"),
dimensions = "date",
anti_sample = TRUE,
segments = NULL,
anti_sample_batches = 1)
View(ga_data)
ga_data <- google_analytics(my_id,
date_range = c(start_date, end_date),
metrics = c("users", "sessions","goal20Completions"),
dimensions = "date", "",
anti_sample = TRUE,
segments = NULL,
anti_sample_batches = 1)
ga_data <- google_analytics(my_id,
date_range = c(start_date, end_date),
metrics = c("users", "sessions","goal20Completions"),
dimensions = "date",
anti_sample = TRUE,
segments = NULL,
anti_sample_batches = 1)
ga_data <- google_analytics(my_id,
date_range = c(start_date, end_date),
metrics = c("users", "sessions","goal20Completions"),
dimensions = "date",
segments = NULL,
anti_sample_batches = 1)
library(googleAnalyticsR)
start_date <- "2018-08-01"
end_date <- "2018-08-31"
my_id <- 56037355
ga_data1 <- google_analytics(my_id,
date_range = c(start_date, end_date),
metrics = c("users", "sessions","goal20Completions"),
dimensions = "date",
anti_sample = TRUE,
segments = NULL,
anti_sample_batches = 1)
ga_data2 <- google_analytics(my_id,
date_range = c(start_date, end_date),
metrics = c("users", "sessions","goal20Completions"),
dimensions = "date",
segments = NULL,
anti_sample_batches = 1)
View(ga_data1)
View(ga_data2)
install.packages("googleAuthR")
install.packages("googleAuthR")
install.packages("googleAuthR")
q()
install.packages("tensor")
remove.packages("tensor", lib="~/R/win-library/3.5")
devtools::install_github("rstudio/keras")
q()
?make_ga_4_req
??make_ga_4_req
today()
Sys.Date()
Sys.Date()-1
?date_range
??date_range
q()
library(swirl)
swirl()
data(cars)
??cars
?cars
head(cars)
plot(cars)
?plot
plot(x = cars$speed, y = cars$dist)
plot(cars, x = cars$dist, y = cars$speed)
plot(x = cars$dist, y = cars$speed)
plot(Speed = cars$speed, y = cars$dist)
?plot
plot(x = cars$speed, y = cars$dist, xlab = "Speed")
plot(x = cars$speed, y = cars$dist, xlab = "Speed", ylab = "Stopping Distance")
plot(x = cars$speed, y = cars$dist, ylab = "Stopping Distance")
plot(x = cars$speed, y = cars$dist, xlab = "Speed", ylab = "Stopping Distance")
plot(x = cars$speed, y = cars$dist, main = "My Plot")
plot(cars, main = "My Plot")
plot(cars, sub = "My Plot Subtitle")
plot(cars, col = 2)
plot(cars, xlim = c(10, 15))
plot(cars, pch = 2)
data("mtcars")
data(mtcars)
?boxplot
boxplot(mpt ~ cyl, data = mtcars)
boxplot(mpg ~ cyl, data = mtcars)
hist(mtcars$mpg)
q()
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
getwd()
setwd("C:/All-Files/Study&Learning/Udemy/Data Science Academy Master Data Science in R/Section1.4")
install.packages("httr")
library(httr)
r <- GET("https://www.nabler.com")
r
View(r\)
View(r)
r[["status_code"]]
r[["headers"]][["server"]]
r[["request"]]
data <- read.table("week4assignment-outcome-of-care-measures.csv", header = TRUE, sep = ",", na.strings = "NA", stringsAsFactors = FALSE)
data <- data[, c(which(colnames(data) == "Hospital.Name"),
which(colnames(data) == "State"),
which(colnames(data) == "Hospital.30.Day.Death..Mortality..Rates.from.Heart.Attack"),
which(colnames(data) == "Hospital.30.Day.Death..Mortality..Rates.from.Heart.Failure"),
which(colnames(data) == "Hospital.30.Day.Death..Mortality..Rates.from.Pneumonia"))]
colnames(data) <- c("hospital.name", "State", "heart.attack", "heart.failure", "pneumonia")
data[data == "Not Available"] <- NA
data$heart.attack <- as.numeric(data$heart.attack)
data$heart.failure <- as.numeric(data$heart.failure)
data$pneumonia <- as.numeric(data$pneumonia)
statelist <- unique(data$State)
outcomelist <- c("heart attack", "heart failure", "pneumonia")
# Check that state and outcome are valid
state <- "AL"
outcome <- "heart attack"
num <- "best"
if(state %in% statelist == FALSE) {
stop("invalid state")
} else if(outcome %in% outcomelist == FALSE) {
stop("invalid outcome")
# If the state and outcome are valid, continue with the program
} else {
# Choose the outcome column to take, subset to the required columns and remove NAs
if(outcome == "heart attack") {
outcomecol <- "heart.attack"
finaldata <- data[, c(1, 2, 3)]
finaldata <- finaldata[complete.cases(finaldata$heart.attack), ]
} else if(outcome == "heart failure") {
outcomecol <- "heart.failure"
finaldata <- data[, c(1, 2, 4)]
finaldata <- finaldata[complete.cases(finaldata$heart.failure), ]
} else {
outcomecol <- "pneumonia"
finaldata <- data[, c(1, 2, 5)]
finaldata <- finaldata[complete.cases(finaldata$pneumonia), ]
}
# Changing best, worst and number function inputs to proper format
if(num == "best") {
num <- 1
} else if(num == "worst") {
num <- nrow(finaldata)
} else if(class(num == "numeric") == TRUE) {
num <- num
}
# Split the data set by state
splitbystate <- split(finaldata, finaldata$State)
View(splitbystate)
ans <- lapply(splitbystate, function(x, num) {
x <- x[order(x[, 3], x$hospital.name), ]
}, num)
View(ans)
return(data.frame(hospital = unlist(ans), state = names(ans)))
}
if(num == "best") {
num <- 1
} else if(num == "worst") {
num <- nrow(finaldata)
} else if(class(num == "numeric") == TRUE) {
num <- num
}
splitbystate <- split(finaldata, finaldata$State)
View(splitbystate)
splitbystate <- split(finaldata, finaldata$State)
if(outcome == "heart attack") {
outcomecol <- "heart.attack"
finaldata <- data[, c(1, 2, 3)]
finaldata <- finaldata[complete.cases(finaldata$heart.attack), ]
} else if(outcome == "heart failure") {
outcomecol <- "heart.failure"
finaldata <- data[, c(1, 2, 4)]
finaldata <- finaldata[complete.cases(finaldata$heart.failure), ]
} else {
outcomecol <- "pneumonia"
finaldata <- data[, c(1, 2, 5)]
finaldata <- finaldata[complete.cases(finaldata$pneumonia), ]
}
if(num == "best") {
num <- 1
} else if(num == "worst") {
num <- nrow(finaldata)
} else if(class(num == "numeric") == TRUE) {
num <- num
}
splitbystate <- split(finaldata, finaldata$State)
requiredPackages <- c("googleAnalyticsR", "tidyr", "dplyr", "openxlsx", "XLConnect")
version()
version
library(fractribution.data)
library(fractribution.model)
path_customer_map
View
View(path_customer_map)
example_path_customer_map
head(example_path_summary)
fractional_attribution <- attribution_fit(example_path_summary)
fractional_attribution <- attribution_fit(example_path_summary, path_level_only = T)
View(fractional_attribution)
install.packages("Forecast")
Y
install.packages("forecast")
install.packages("lubridate")
install.packages("dplyr")
install.packages("ttr")
install.packages("TTR")
install.packages("DMWR")
install.packages("DMwR")
install.packages("Tseries")
install.packages("tseries")
exit
q()
if(require(forecast)){library(forecast)} else {install.packages("forecast");library(forecast)}
if(require(tseries)){library(tseries)} else {install.packages("tseries");library(tseries)}
if(require(aTSA)){library(aTSA)} else {install.packages("aTSA");library(aTSA)}
if(require(plotly)){library(plotly)} else {install.packages("plotly");library(plotly)}
if(require(Hmisc)){library(Hmisc)} else {install.packages("Hmisc");library(Hmisc)}
if(require(dplyr)){library(dplyr)} else {install.packages("dplyr");library(dplyr)}
library(knitr)
library(tm)
library(forecast)
data<-read.csv(file.choose(),header=T,na.strings = c(""," "),stringsAsFactors = F)
data<-read.csv(file.choose(),header=T,na.strings = c(""," "),stringsAsFactors = F)
data$Month <- factor(data$Month, levels = data[["Month"]])
#Splitting into test and train by logical split by seasonality (for example, complete year)
data_train<-data[1:72,2]
data_test<-data[73:84,2]
#Converting data to a time-series
volume_ts<-ts(data_train,start = c(2009,1),end=c(2014,12),frequency = 12)
# Decomposing to look at the seasonal, trend and irregular components
comp <- stl(volume_ts, s.window = "periodic")
plot(comp)
plot.ts(volume_ts)
# Performing a Unit Root Test - Augmented Dickey Fuller (ADF) test
tseries::adf.test(volume_ts)
tseries::adf.test(diff(volume_ts,differences = 1))
plot(diff(volume_ts),ylab="Differenced",type="l")
# Decomposing to look at the seasonal, trend and irregular components
comp <- stl(volume_ts, s.window = "periodic")
plot(comp)
plot.ts(volume_ts)
# Performing a Unit Root Test - Augmented Dickey Fuller (ADF) test
tseries::adf.test(volume_ts)
tseries::adf.test(diff(volume_ts,differences = 1))
diff(volume_ts)
?tseries
??tseries
?adf.test
install.packages("rvest")
library(rvest)
?httr::status_code()
options(scipen = 999)
options(scipen = 999)
4000/1386400000
alarm()
alarm()
install.packages("beepr")
library(beepr)
beep()
beep()
?beep
beep(4)
beep(5)
beep(7)
beep(9)
beep(8)
beep(11)
getwd()
setwd("C:/All-Files/Study&Learning/Coursera/DataScience/GettingAndCleaningData/Week2")
getwd()
install.packages("RMySQL")
library(RMySQL)
# dbConnect command helps connect with the database
ucscDB <- dbConnect(MySQL(), user = "genome",
host = "genome-msql.cse.ucsc.edu")
# dbConnect command helps connect with the database
ucscDB <- dbConnect(MySQL(), user = "genome",
host = "genome-msql.soe.ucsc.edu")
ucscDb <- dbConnect(MySQL(), user = "genome", host="genome-mysql.soe.ucsc.edu")
ucscDb <- dbConnect(MySQL(), user = "genome",
host = "genome-mysql.soe.ucsc.edu")
result <- dbGetQuery(ucscDb, "show databases: "); dbDisconnect(ucscDb);
result <- dbGetQuery(ucscDb, "show databases;"); dbDisconnect(ucscDb);
result <- dbGetQuery(ucscDb, "show databases; "); dbDisconnect(ucscDb);
result <- dbGetQuery(ucscDb, "show databases; "); dbDisconnect(ucscDb);
result <- dbGetQuery(ucscDb, "show databases;"); dbDisconnect(ucscDb);
result <- dbGetQuery(ucscDb, "show databases;"); dbDisconnect(ucscDb);
result <- dbGetQuery(ucscDb, "show databases;"); dbDisconnect(ucscDb);
# dbConnect command helps connect with the database
ucscDb <- dbConnect(MySQL(), user = "genome",
host = "genome-mysql.soe.ucsc.edu")
result <- dbGetQuery(ucscDb, "show databases;"); dbDisconnect(ucscDb);
?dbConnect
result
# Connecting to one database and getting tables
hg19 <- dbConnect(MySQL(), user = "genome",
db = "hg19",
host = "genome-mysql.soe.ucsc.edu")
allTables[1:5]
allTables <- dbListTables(hg19)
length(allTables)
allTables[1:5]
# Get all the fields in a particular table
dbListFields(hg19, "affyU133Plus2")
# Names of the first 5 tables
allTables[1:]
# Names of the first 5 tables
allTables[1:10]
View(allTables)
# Get all the fields in a particular table
dbListFields(hg19, "affyU133Plus2")
# dbConnect command helps connect with the database
ucscDb <- dbConnect(MySQL(), user = "genome",
host = "genome-mysql.soe.ucsc.edu")
# dbGetQuery will help run mysql commands. After running query, alwaysdisconnect.
result <- dbGetQuery(ucscDb, "show databases;"); dbDisconnect(ucscDb);
result
# Connecting to one database and getting tables
hg19 <- dbConnect(MySQL(), user = "genome",
db = "hg19",
host = "genome-mysql.soe.ucsc.edu")
allTables <- dbListTables(hg19)
# Find the number of tables in the database
length(allTables)
# Names of the first 5 tables
allTables[1:10]
View(allTables)
# Get all the fields in a particular table
dbListFields(hg19, "affyU133Plus2")
dbGetQuery(hg19, "select count(*) from affyU133Plus2")
# Read from a table into R
affyData <- dbReadTable(hg19, "affyU133Plus2")
# Get all the fields in a particular table
dbListFields(hg19, "affyU133Plus2")
# dbGetQuery will help run mysql commands. After running query, alwaysdisconnect.
result <- dbGetQuery(ucscDb, "show databases;"); dbDisconnect(ucscDb);
# dbConnect command helps connect with the database
ucscDb <- dbConnect(MySQL(), user = "genome",
host = "genome-mysql.soe.ucsc.edu")
# dbGetQuery will help run mysql commands. After running query, alwaysdisconnect.
result <- dbGetQuery(ucscDb, "show databases;"); dbDisconnect(ucscDb);
# Connecting to one database and getting tables
hg19 <- dbConnect(MySQL(), user = "genome",
db = "hg19",
host = "genome-mysql.soe.ucsc.edu")
allTables <- dbListTables(hg19)
# Get all the fields in a particular table
dbListFields(hg19, "affyU133Plus2")
# Count of all the records
dbGetQuery(hg19, "select count(*) from affyU133Plus2")
# Read from a table into R
affyData <- dbReadTable(hg19, "affyU133Plus2")
head(affyData)
warnings()
# Read from a table into R
affyData <- dbReadTable(hg19, "affyU133Plus2")
head(affyData)
View(affyData)
# Select subset from mySQL
query <- dbSendQuery(hg19, "select * from affyU133Plus2 where misMatches between 1 and 3")
# Connecting to one database and getting tables
hg19 <- dbConnect(MySQL(), user = "genome",
db = "hg19",
host = "genome-mysql.soe.ucsc.edu")
# Select subset from mySQL
query <- dbSendQuery(hg19, "select * from affyU133Plus2 where misMatches between 1 and 3")
affyMis <- fetch(query); quantile(aaffyMis$misMatches)
# Select subset from mySQL
query <- dbSendQuery(hg19, "select * from affyU133Plus2 where misMatches between 1 and 3")
# Connecting to one database and getting tables
hg19 <- dbConnect(MySQL(), user = "genome",
db = "hg19",
host = "genome-mysql.soe.ucsc.edu")
# Select subset from mySQL
query <- dbSendQuery(hg19, "select * from affyU133Plus2 where misMatches between 1 and 3")
affyMis <- fetch(query); quantile(aaffyMis$misMatches)
affyMis <- fetch(query); quantile(affyMis$misMatches)
# Bring in a very small subset of 10 rows from table
affyMisSmall <- fetch(query, n = 10); dbClearResult(query);
dim(affyMisSmall)
# Bring in a very small subset of 10 rows from table
affyMisSmall <- fetch(query, n = 10); dbClearResult(query);
dim(affyMisSmall)
# Bring in a very small subset of 10 rows from table
affyMisSmall <- fetch(query, n = 10); dbClearResult(query)
# Bring in a very small subset of 10 rows from table
affyMisSmall <- fetch(query, n = 10); dbClearResult(query);
# Select subset from mySQL
query <- dbSendQuery(hg19, "select * from affyU133Plus2 where misMatches between 1 and 3")
# Connecting to one database and getting tables
hg19 <- dbConnect(MySQL(), user = "genome",
db = "hg19",
host = "genome-mysql.soe.ucsc.edu")
# Bring in a very small subset of 10 rows from table
affyMisSmall <- fetch(query, n = 10); dbClearResult(query);
dim(affyMisSmall)
# Install R HDF5 Package
source("http://bioconductor.org/biocLite.R")
biocLite("rdhf5")
library(beepr)
?beepr
beep(sound = 8)
library(rhdf5)
# Install R HDF5 Package
source("http://bioconductor.org/biocLite.R")
biocLite("rdhf5")
install.packages("BiocManager")
BiocManager::install("rhdf5")
library(rhdf5)
created <- h5createFile("example.h5")
created
getwd()
# Create groups
created <- h5createGroup("example.h5", "foo")
created <- h5createGroup("example.h5", "baa")
created <- h5createGroup("example.h5", "foo/foobaa")
h5ls("example.h5")
# Write to groups
A <- matrix(1:10, nrow = 5, ncol = 2)
h5write(A, "example.h5", "fo/A")
h5write(A, "example.h5", "foo/A")
B <- array(seq(0.1, 2.0, by = 0.1), dim = c(5, 2, 2))
attr(B, "scale") <- "liter"
h5write(B, "example.h5", "foo/foobaa/B")
h5ls("example.h5")
# Write a data set directly
df <- data.frame(1L:5L, seq(0, 1, length.out = 5),
c("ab", "cde", "fghi", "a", "s"),
stringsAsFactors = FALSE)
h5write(df, "example.h5", "df")
d5ls("example.h5")
h5ls("example.h5")
getwd()
# Reading data
readA <- h5read("example.h5", "foo/A")
readB <- h5read("example.h5", "foo/foobaa/B")
readdf <- h5read("example.h5", "df")
readA
# Writing and reading chunks
h5write(c(12, 13, 14), "example.h5", "foo/A", index = list(1:3, 1))
h5read("example.h5", "foo/A")
# Webscraping
# Webscraping is programmatically extracting data from the HTML code of websites
# Getting data off webpages using readLines()
con <- url("https://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
htmlCode <- readLines(con)
close(con)
htmlCode
htmlCode
# Parsing through XML
library(XML)
url <- "https://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html <- htmlTreeParse(url, useInternalNodes = T)
xpathSApply(html, "//title", xmlValue)
url <- "http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html <- htmlTreeParse(url, useInternalNodes = T, )
xpathSApply(html, "//title", xmlValue)
html <- htmlTreeParse(url, useInternalNodes = T)
theurl <- "https://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
fileURL <- getURL(url = theurl)
library(RCurl)
fileURL <- getURL(url = theurl)
html <- htmlTreeParse(fileurl, useInternalNodes = T)
html <- htmlTreeParse(fileURL, useInternalNodes = T)
xpathSApply(html, "//title", xmlValue)
xpathSApply(html, "//td[@id = 'col-citedby']", xmlValue)
xpathSApply(html, "//title", xmlValue)
xpathSApply(html, "//td[@id = 'col-citedby']", xmlValue)
# code below not working
xpathSApply(html, "//td[@class = 'gsc_a_ac gs_ibl']", xmlValue)
# Using httr package
library(httr)
html2 <- GET(theurl)
content2 <- content(html2, as = "text")
parsedHTML <- htmlParse(content2, asText = TRUE)
xpathSApply(parsedHTML, "//tile", xmlValue)
html2 <- GET(fileURL)
?GET
html2 <- GET(theURL)
html2 <- GET(theurl)
content2 <- content(html2, as = "text")
parsedHTML <- htmlParse(content2, asText = TRUE)
xpathSApply(parsedHTML, "//tile", xmlValue)
parsedHTML
xpathSApply(parsedHTML, "//title", xmlValue)
# Accessing websites with passwords
pg1 <- GET("http://httpbin.org/basic-auth/user/passwd")
pg1
# Using httr to authenticate with credentials
pg2 <- GET("http://httpbin.org/basic-auth/user/passwd",
authenticate("user", "passwd"))
pg2
names(pg2)
# Use handles to use one time authentication
google <- handle("https://www.google.com")
pg1 <- GET(handle = google, path = "/")
pg2 <- GET(handle = google, path = "search")
pg1
pg2
